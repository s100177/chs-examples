#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬ - æµ‹è¯•æ‰€æœ‰ä»¿çœŸè¿è¡Œæ–¹å¼

æœ¬è„šæœ¬è‡ªåŠ¨æµ‹è¯•CHS-SDKä¸­çš„æ‰€æœ‰ä»¿çœŸè¿è¡Œæ–¹å¼ï¼ŒåŒ…æ‹¬ï¼š
1. run_hardcoded.py - ç¡¬ç¼–ç ç¤ºä¾‹è¿è¡Œå™¨
2. run_unified_scenario.py - ç»Ÿä¸€é…ç½®æ–‡ä»¶æ–¹å¼
3. run_scenario.py - ä¼ ç»Ÿå¤šé…ç½®æ–‡ä»¶æ–¹å¼
4. run_universal_config.py - é€šç”¨é…ç½®è¿è¡Œå™¨

è¿è¡Œæ–¹å¼ï¼š
    python test_all_simulation_modes.py
"""

import sys
import os
import subprocess
import time
from pathlib import Path
from typing import Dict, List, Tuple

# è®¾ç½®çŽ¯å¢ƒå˜é‡å¼ºåˆ¶ä½¿ç”¨UTF-8ç¼–ç 
os.environ['PYTHONIOENCODING'] = 'utf-8'
os.environ['PYTHONUTF8'] = '1'

class SimulationModesTester:
    """ä»¿çœŸæ¨¡å¼æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.examples_dir = Path(__file__).parent
        self.test_results = {}
        self.total_tests = 0
        self.passed_tests = 0
        self.failed_tests = 0
    
    def run_command_test(self, command: List[str], test_name: str, timeout: int = 60) -> Tuple[bool, str, float]:
        """Run command test"""
        print(f"\n=== Testing {test_name} ===")
        print(f"Command: {' '.join(command)}")
        print(f"Working directory: {self.examples_dir}")
        print(f"Timeout setting: {timeout} seconds")
        print("Starting execution...")
        
        start_time = time.time()
        try:
            # è®¾ç½®å­è¿›ç¨‹çŽ¯å¢ƒå˜é‡ï¼Œå¼ºåˆ¶ä½¿ç”¨UTF-8ç¼–ç 
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            env['PYTHONUTF8'] = '1'
            
            # ä½¿ç”¨subprocess.runæ¥ç®€åŒ–å¤„ç†å¹¶ç¡®ä¿æ­£ç¡®çš„ç¼–ç 
            result = subprocess.run(
                command,
                cwd=str(self.examples_dir),
                capture_output=True,
                text=True,
                timeout=timeout,
                check=False,
                encoding='utf-8',
                errors='replace',  # æ›¿æ¢æ— æ³•è§£ç çš„å­—ç¬¦ï¼Œé¿å…ä¹±ç 
                env=env  # ä¼ é€’çŽ¯å¢ƒå˜é‡
            )
            
            end_time = time.time()
            execution_time = end_time - start_time
            
            print(f"Execution completed, time taken: {execution_time:.2f} seconds")
            print(f"Return code: {result.returncode}")
            print(f"Standard output length: {len(result.stdout)} characters")
            print(f"Standard error length: {len(result.stderr)} characters")
            
            # æ¸…ç†è¾“å‡ºä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œé¿å…ä¹±ç 
            clean_stdout = result.stdout.replace('\x00', '').strip() if result.stdout else ""
            clean_stderr = result.stderr.replace('\x00', '').strip() if result.stderr else ""
            
            if result.returncode == 0:
                print(f"âœ“ Test passed")
                if clean_stdout:
                    print(f"Standard output: {clean_stdout[:200]}..." if len(clean_stdout) > 200 else f"Standard output: {clean_stdout}")
                return True, clean_stdout, execution_time
            else:
                print(f"âœ— Test failed (return code: {result.returncode})")
                error_output = clean_stderr or clean_stdout or "No error output"
                if error_output:
                    print(f"Error output: {error_output[:200]}..." if len(error_output) > 200 else f"Error output: {error_output}")
                return False, error_output, execution_time
                
        except subprocess.TimeoutExpired:
            execution_time = time.time() - start_time
            print(f"âœ— Test timeout (>{timeout} seconds)")
            return False, "Test timeout", execution_time
        except Exception as e:
            execution_time = time.time() - start_time
            print(f"âœ— Test exception: {e}")
            return False, str(e), execution_time
    
    def test_hardcoded_runner(self) -> bool:
        """Test hardcoded runner - run all available examples"""
        print("ðŸ“‹ Testing all hardcoded examples (built-in configuration)")
        
        # èŽ·å–æ‰€æœ‰å¯ç”¨çš„ä¾‹å­åˆ—è¡¨
        examples = {
            "getting_started": "å…¥é—¨ç¤ºä¾‹ - åŸºç¡€æ°´åº“-é—¸é—¨ç³»ç»Ÿä»¿çœŸ",
            "multi_component": "å¤šç»„ä»¶ç³»ç»Ÿ - å¤æ‚å¤šç»„ä»¶æ°´åˆ©ç³»ç»Ÿä»¿çœŸ",
            "event_driven_agents": "äº‹ä»¶é©±åŠ¨æ™ºèƒ½ä½“ - åŸºäºŽäº‹ä»¶çš„æ™ºèƒ½ä½“æŽ§åˆ¶ç³»ç»Ÿ",
            "hierarchical_control": "åˆ†å±‚æŽ§åˆ¶ - åˆ†å±‚åˆ†å¸ƒå¼æŽ§åˆ¶ç³»ç»Ÿ",
            "complex_networks": "å¤æ‚ç½‘ç»œ - åˆ†æ”¯ç½‘ç»œç³»ç»Ÿä»¿çœŸ",
            "pump_station": "æ³µç«™æŽ§åˆ¶ - æ³µç«™æ™ºèƒ½æŽ§åˆ¶ç³»ç»Ÿ",
            "hydropower_plant": "æ°´ç”µç«™ - æ°´ç”µç«™è¿è¡Œä»¿çœŸ",
            "canal_pid_control": "æ¸ é“PIDæŽ§åˆ¶ - æ¸ é“ç³»ç»ŸPIDæŽ§åˆ¶å¯¹æ¯”",
            "canal_mpc_control": "æ¸ é“MPCæŽ§åˆ¶ - æ¸ é“ç³»ç»ŸMPCä¸ŽPIDæŽ§åˆ¶",
            "reservoir_identification": "æ°´åº“å‚æ•°è¾¨è¯† - æ°´åº“åº“å®¹æ›²çº¿å‚æ•°è¾¨è¯†",
            "simplified_demo": "ç®€åŒ–æ¼”ç¤º - ç®€åŒ–æ°´åº“æŽ§åˆ¶æ¼”ç¤º",
            "mission_example_1": "ä»»åŠ¡ç¤ºä¾‹1 - åŸºç¡€ç‰©ç†ä¸Žé«˜çº§æŽ§åˆ¶",
            "mission_example_2": "ä»»åŠ¡ç¤ºä¾‹2 - é—­çŽ¯æŽ§åˆ¶ç³»ç»Ÿ",
            "mission_example_3": "ä»»åŠ¡ç¤ºä¾‹3 - å¢žå¼ºæ„ŸçŸ¥ç³»ç»Ÿ",
            "mission_example_5": "ä»»åŠ¡ç¤ºä¾‹5 - æ¶¡è½®é—¸é—¨ä»¿çœŸ",
            "mission_scenarios": "Missionåœºæ™¯ç¤ºä¾‹ - ä»Žmissionç›®å½•è¿ç§»çš„åœºæ™¯ç¤ºä¾‹"
        }
        
        all_success = True
        total_time = 0
        all_outputs = []
        failed_examples = []
        
        print(f"\næ€»å…±éœ€è¦æµ‹è¯• {len(examples)} ä¸ªç¡¬ç¼–ç ä¾‹å­:")
        
        for i, (example_key, example_desc) in enumerate(examples.items(), 1):
            print(f"\n[{i}/{len(examples)}] ðŸ”„ æ­£åœ¨æµ‹è¯•: {example_desc}")
            print(f"ç¤ºä¾‹é”®å: {example_key}")
            
            command = ["python", "-u", "run_hardcoded.py", "--example", example_key]
            success, output, exec_time = self.run_command_test(command, f"Hardcoded Example: {example_key}", timeout=120)
            
            total_time += exec_time
            all_outputs.append(f"=== {example_key} ===\n{output}")
            
            if success:
                print(f"âœ… [{i}/{len(examples)}] {example_desc} - æµ‹è¯•é€šè¿‡ ({exec_time:.2f}ç§’)")
            else:
                print(f"âŒ [{i}/{len(examples)}] {example_desc} - æµ‹è¯•å¤±è´¥ ({exec_time:.2f}ç§’)")
                failed_examples.append(example_key)
                all_success = False
        
        # æ±‡æ€»ç»“æžœ
        print(f"\n=== ç¡¬ç¼–ç ä¾‹å­æµ‹è¯•æ±‡æ€» ===")
        print(f"æ€»æµ‹è¯•æ•°é‡: {len(examples)}")
        print(f"æˆåŠŸæ•°é‡: {len(examples) - len(failed_examples)}")
        print(f"å¤±è´¥æ•°é‡: {len(failed_examples)}")
        print(f"æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}ç§’")
        
        if failed_examples:
            print(f"å¤±è´¥çš„ä¾‹å­: {', '.join(failed_examples)}")
        
        self.test_results["run_hardcoded"] = {
            "success": all_success,
            "output": "\n\n".join(all_outputs),
            "execution_time": total_time,
            "description": f"Hardcoded Example Runner - {len(examples)} examples tested",
            "failed_examples": failed_examples,
            "total_examples": len(examples)
        }
        
        return all_success
    
    def test_unified_scenario_runner(self) -> bool:
        """Test unified scenario runner - run all available examples"""
        print("ðŸ“‹ Testing all unified scenario examples (ç»Ÿä¸€é…ç½®æ–‡ä»¶æ–¹æ³•)")
        
        # åŠ¨æ€èŽ·å–æ‰€æœ‰å¯ç”¨çš„ä¾‹å­åˆ—è¡¨
        try:
            sys.path.insert(0, str(self.examples_dir))
            from run_unified_scenario import ExamplesUnifiedScenarioRunner
            
            runner = ExamplesUnifiedScenarioRunner()
            available_examples = runner._discover_examples()
            
            # è¿‡æ»¤å‡ºæœ‰æœ‰æ•ˆé…ç½®æ–‡ä»¶çš„ç¤ºä¾‹
            examples = {}
            for key, example_info in available_examples.items():
                example_path = self.examples_dir / Path(example_info['path']).parent
                universal_config = example_path / 'universal_config.yml'
                traditional_config = example_path / 'config.yml'
                
                # åªåŒ…å«æœ‰é…ç½®æ–‡ä»¶çš„ç¤ºä¾‹
                if universal_config.exists() or traditional_config.exists():
                    examples[key] = example_info['description']
                
        except Exception as e:
            print(f"âŒ æ— æ³•èŽ·å–ç»Ÿä¸€åœºæ™¯ç¤ºä¾‹åˆ—è¡¨: {e}")
            return False
        
        all_success = True
        total_time = 0
        all_outputs = []
        failed_examples = []
        
        print(f"\næ€»å…±éœ€è¦æµ‹è¯• {len(examples)} ä¸ªç»Ÿä¸€åœºæ™¯ä¾‹å­:")
        
        for i, (example_key, example_desc) in enumerate(examples.items(), 1):
            print(f"\n[{i}/{len(examples)}] ðŸ”„ æ­£åœ¨æµ‹è¯•: {example_desc}")
            print(f"ç¤ºä¾‹é”®å: {example_key}")
            
            command = ["python", "-u", "run_unified_scenario.py", "--example", example_key]
            success, output, exec_time = self.run_command_test(command, f"Unified Scenario: {example_key}", timeout=120)
            
            total_time += exec_time
            all_outputs.append(f"=== {example_key} ===\n{output}")
            
            if success:
                print(f"âœ… [{i}/{len(examples)}] {example_desc} - æµ‹è¯•é€šè¿‡ ({exec_time:.2f}ç§’)")
            else:
                print(f"âŒ [{i}/{len(examples)}] {example_desc} - æµ‹è¯•å¤±è´¥ ({exec_time:.2f}ç§’)")
                failed_examples.append(example_key)
                all_success = False
        
        # æ±‡æ€»ç»“æžœ
        print(f"\n=== ç»Ÿä¸€åœºæ™¯ä¾‹å­æµ‹è¯•æ±‡æ€» ===")
        print(f"æ€»æµ‹è¯•æ•°é‡: {len(examples)}")
        print(f"æˆåŠŸæ•°é‡: {len(examples) - len(failed_examples)}")
        print(f"å¤±è´¥æ•°é‡: {len(failed_examples)}")
        print(f"æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}ç§’")
        
        if failed_examples:
            print(f"å¤±è´¥çš„ä¾‹å­: {', '.join(failed_examples)}")
        
        self.test_results["run_unified_scenario"] = {
            "success": all_success,
            "output": "\n\n".join(all_outputs),
            "execution_time": total_time,
            "description": f"Unified Configuration File Method - {len(examples)} examples tested",
            "failed_examples": failed_examples,
            "total_examples": len(examples)
        }
        
        return all_success
    
    def test_scenario_runner(self) -> bool:
        """Test traditional scenario runner - run all available examples"""
        print("ðŸ“‹ Testing all traditional scenario examples (ä¼ ç»Ÿå¤šé…ç½®æ–‡ä»¶æ–¹æ³•)")
        
        # ä»Žrun_scenario.pyèŽ·å–æ‰€æœ‰å¯ç”¨ç¤ºä¾‹
        try:
            sys.path.insert(0, str(self.examples_dir))
            from run_scenario import ExamplesScenarioRunner
            
            runner = ExamplesScenarioRunner()
            examples = runner.list_examples()
            
            # è½¬æ¢ä¸ºæµ‹è¯•æ ¼å¼ï¼ŒåªåŒ…å«æœ‰å®Œæ•´å¤šé…ç½®æ–‡ä»¶çš„ç¤ºä¾‹
            test_examples = {}
            for example_key, example in examples.items():
                # æž„å»ºåœºæ™¯è·¯å¾„
                scenario_path = self.examples_dir / example['path']
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¼ ç»Ÿå¤šé…ç½®æ–‡ä»¶æ–¹æ³•éœ€è¦çš„æ–‡ä»¶
                config_file = scenario_path / 'config.yml'
                components_file = scenario_path / 'components.yml'
                topology_file = scenario_path / 'topology.yml'
                
                # åªåŒ…å«æœ‰å®Œæ•´å¤šé…ç½®æ–‡ä»¶çš„ç¤ºä¾‹ï¼ˆè‡³å°‘è¦æœ‰config.ymlå’Œcomponents.ymlï¼‰
                if config_file.exists() and components_file.exists():
                    test_examples[example_key] = {
                        'desc': f"{example['name']} - {example['description']}",
                        'path': str(scenario_path),
                        'config': 'config.yml'
                    }
            
            # å¦‚æžœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å®Œæ•´çš„å¤šé…ç½®æ–‡ä»¶ç¤ºä¾‹ï¼Œè·³è¿‡æµ‹è¯•
            if not test_examples:
                print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°åŒ…å«å®Œæ•´å¤šé…ç½®æ–‡ä»¶çš„ç¤ºä¾‹ï¼Œè·³è¿‡ä¼ ç»Ÿå¤šé…ç½®æ–‡ä»¶æ–¹æ³•æµ‹è¯•")
                self.test_results["run_scenario"] = {
                    "success": True,
                    "output": "No multi-config examples found, test skipped",
                    "execution_time": 0.0,
                    "description": "Traditional Multi-Configuration File Method - 0 examples tested (skipped)",
                    "failed_examples": [],
                    "total_examples": 0
                }
                return True
            
        except Exception as e:
            print(f"âŒ æ— æ³•èŽ·å–ä¼ ç»Ÿåœºæ™¯ç¤ºä¾‹åˆ—è¡¨: {e}")
            # å¦‚æžœæ— æ³•åŠ¨æ€èŽ·å–ï¼Œè·³è¿‡ä¼ ç»Ÿå¤šé…ç½®æ–‡ä»¶æ–¹æ³•æµ‹è¯•
            # å› ä¸ºå¤§å¤šæ•°ç¤ºä¾‹åªæœ‰å•ä¸ªconfig.ymlæ–‡ä»¶ï¼Œä¸é€‚åˆä¼ ç»Ÿå¤šé…ç½®æ–‡ä»¶æ–¹æ³•
            print("âš ï¸  ä¼ ç»Ÿå¤šé…ç½®æ–‡ä»¶æ–¹æ³•éœ€è¦å®Œæ•´çš„å¤šé…ç½®æ–‡ä»¶ï¼Œå¤§å¤šæ•°ç¤ºä¾‹ä¸å…¼å®¹ï¼Œè·³è¿‡æµ‹è¯•")
            self.test_results["run_scenario"] = {
                "success": True,
                "output": "Traditional multi-config method skipped - most examples use single config.yml",
                "execution_time": 0.0,
                "description": "Traditional Multi-Configuration File Method - 0 examples tested (skipped)",
                "failed_examples": [],
                "total_examples": 0
            }
            return True
        
        # éªŒè¯è¿™äº›ç¤ºä¾‹ç¡®å®žå­˜åœ¨ä¸”å…·æœ‰å®Œæ•´çš„å¤šé…ç½®æ–‡ä»¶ç»“æž„
        filtered_examples = {}
        for key, info in test_examples.items():
            scenario_path = self.examples_dir / info['path']
            config_file = scenario_path / 'config.yml'
            components_file = scenario_path / 'components.yml'
            topology_file = scenario_path / 'topology.yml'
            agents_file = scenario_path / 'agents.yml'
            
            # ä¼ ç»Ÿå¤šé…ç½®æ–‡ä»¶æ–¹æ³•éœ€è¦å®Œæ•´çš„é…ç½®æ–‡ä»¶ç»“æž„
            if (config_file.exists() and components_file.exists() and 
                topology_file.exists() and agents_file.exists()):
                filtered_examples[key] = info
        test_examples = filtered_examples
        
        all_success = True
        total_time = 0
        all_outputs = []
        failed_examples = []
        
        print(f"\næ€»å…±éœ€è¦æµ‹è¯• {len(test_examples)} ä¸ªä¼ ç»Ÿåœºæ™¯ä¾‹å­:")
        
        for i, (example_key, example_info) in enumerate(test_examples.items(), 1):
            print(f"\n[{i}/{len(test_examples)}] ðŸ”„ æ­£åœ¨æµ‹è¯•: {example_info['desc']}")
            print(f"ç¤ºä¾‹é”®å: {example_key}")
            print(f"åœºæ™¯è·¯å¾„: {example_info['path']}")
            
            # æž„å»ºå‘½ä»¤ - ä½¿ç”¨--exampleå‚æ•°è€Œä¸æ˜¯ç›´æŽ¥ä¼ é€’è·¯å¾„
            command = ["python", "-u", "run_scenario.py", "--example", example_key]
            success, output, exec_time = self.run_command_test(command, f"Traditional Scenario: {example_key}", timeout=120)
            
            total_time += exec_time
            all_outputs.append(f"=== {example_key} ===\n{output}")
            
            if success:
                print(f"âœ… [{i}/{len(test_examples)}] {example_info['desc']} - æµ‹è¯•é€šè¿‡ ({exec_time:.2f}ç§’)")
            else:
                print(f"âŒ [{i}/{len(test_examples)}] {example_info['desc']} - æµ‹è¯•å¤±è´¥ ({exec_time:.2f}ç§’)")
                failed_examples.append(example_key)
                all_success = False
        
        # æ±‡æ€»ç»“æžœ
        print(f"\n=== ä¼ ç»Ÿåœºæ™¯ä¾‹å­æµ‹è¯•æ±‡æ€» ===")
        print(f"æ€»æµ‹è¯•æ•°é‡: {len(test_examples)}")
        print(f"æˆåŠŸæ•°é‡: {len(test_examples) - len(failed_examples)}")
        print(f"å¤±è´¥æ•°é‡: {len(failed_examples)}")
        print(f"æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}ç§’")
        
        if failed_examples:
            print(f"å¤±è´¥çš„ä¾‹å­: {', '.join(failed_examples)}")
        
        self.test_results["run_scenario"] = {
            "success": all_success,
            "output": "\n\n".join(all_outputs),
            "execution_time": total_time,
            "description": f"Traditional Multi-Configuration File Method - {len(test_examples)} examples tested",
            "failed_examples": failed_examples,
            "total_examples": len(test_examples)
        }
        
        return all_success
    
    def test_universal_config_runner(self) -> bool:
        """Test universal configuration runner - run all available examples"""
        print("ðŸ“‹ Testing all universal configuration examples (é€šç”¨é…ç½®è¿è¡Œå™¨)")
        
        # åŠ¨æ€èŽ·å–æ‰€æœ‰å¯ç”¨çš„ä¾‹å­åˆ—è¡¨
        try:
            sys.path.insert(0, str(self.examples_dir))
            from run_universal_config import ExamplesUniversalConfigRunner
            
            runner = ExamplesUniversalConfigRunner()
            available_examples = runner._discover_examples()
            
            # è½¬æ¢ä¸ºæµ‹è¯•æ ¼å¼ï¼ŒåªåŒ…å«æœ‰universal_config.ymlçš„ç¤ºä¾‹
            examples = {}
            for key, example_info in available_examples.items():
                # æž„å»ºç¤ºä¾‹è·¯å¾„
                example_path = self.examples_dir / example_info['path']
                universal_config_file = example_path / 'universal_config.yml'
                
                # åªåŒ…å«æœ‰universal_config.ymlæ–‡ä»¶çš„ç¤ºä¾‹
                if universal_config_file.exists():
                    examples[key] = example_info['description']
            
            # å¦‚æžœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•universal_config.ymlç¤ºä¾‹ï¼Œè·³è¿‡æµ‹è¯•
            if not examples:
                print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°åŒ…å«universal_config.ymlçš„ç¤ºä¾‹ï¼Œè·³è¿‡é€šç”¨é…ç½®è¿è¡Œå™¨æµ‹è¯•")
                self.test_results["run_universal_config"] = {
                    "success": True,
                    "output": "No universal config examples found, test skipped",
                    "execution_time": 0.0,
                    "description": "Universal Config Runner - 0 examples tested (skipped)",
                    "failed_examples": [],
                    "total_examples": 0
                }
                return True
                
        except Exception as e:
            print(f"âŒ æ— æ³•èŽ·å–é€šç”¨é…ç½®ç¤ºä¾‹åˆ—è¡¨: {e}")
            return False
        
        all_success = True
        total_time = 0
        all_outputs = []
        failed_examples = []
        
        print(f"\næ€»å…±éœ€è¦æµ‹è¯• {len(examples)} ä¸ªé€šç”¨é…ç½®ä¾‹å­ (ä»…åŒ…å«universal_config.ymlçš„ç¤ºä¾‹):")
        
        for i, (example_key, example_desc) in enumerate(examples.items(), 1):
            print(f"\n[{i}/{len(examples)}] ðŸ”„ æ­£åœ¨æµ‹è¯•: {example_desc}")
            print(f"ç¤ºä¾‹é”®å: {example_key}")
            
            command = ["python", "-u", "run_universal_config.py", "--example", example_key]
            success, output, exec_time = self.run_command_test(command, f"Universal Config: {example_key}", timeout=120)
            
            total_time += exec_time
            all_outputs.append(f"=== {example_key} ===\n{output}")
            
            if success:
                print(f"âœ… [{i}/{len(examples)}] {example_desc} - æµ‹è¯•é€šè¿‡ ({exec_time:.2f}ç§’)")
            else:
                print(f"âŒ [{i}/{len(examples)}] {example_desc} - æµ‹è¯•å¤±è´¥ ({exec_time:.2f}ç§’)")
                failed_examples.append(example_key)
                all_success = False
        
        # æ±‡æ€»ç»“æžœ
        print(f"\n=== é€šç”¨é…ç½®ä¾‹å­æµ‹è¯•æ±‡æ€» ===")
        print(f"æ€»æµ‹è¯•æ•°é‡: {len(examples)}")
        print(f"æˆåŠŸæ•°é‡: {len(examples) - len(failed_examples)}")
        print(f"å¤±è´¥æ•°é‡: {len(failed_examples)}")
        print(f"æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}ç§’")
        
        if failed_examples:
            print(f"å¤±è´¥çš„ä¾‹å­: {', '.join(failed_examples)}")
        
        self.test_results["run_universal_config"] = {
            "success": all_success,
            "output": "\n\n".join(all_outputs),
            "execution_time": total_time,
            "description": f"Universal Configuration Runner - {len(examples)} examples tested",
            "failed_examples": failed_examples,
            "total_examples": len(examples)
        }
        
        return all_success
    
    def run_all_tests(self) -> None:
        """Run all tests"""
        print("=== CHS-SDK Simulation Mode Automated Testing ===")
        print(f"Test directory: {self.examples_dir}")
        print(f"Start time: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Define test list
        tests = [
            ("Hardcoded Example Runner", self.test_hardcoded_runner),
            ("Unified Configuration File Method", self.test_unified_scenario_runner),
            ("Traditional Multi-Configuration File Method", self.test_scenario_runner),
            ("Universal Configuration Runner", self.test_universal_config_runner)
        ]
        
        self.total_tests = len(tests)
        print(f"\nTotal tests to run: {self.total_tests}")
        print("="*60)
        
        # Run tests
        for i, (test_name, test_func) in enumerate(tests, 1):
            print(f"\n[{i}/{self.total_tests}] Preparing to run test: {test_name}")
            print(f"Current progress: {((i-1)/self.total_tests)*100:.1f}%")
            
            test_start_time = time.time()
            try:
                if test_func():
                    self.passed_tests += 1
                    print(f"[{i}/{self.total_tests}] âœ“ {test_name} test passed")
                else:
                    self.failed_tests += 1
                    print(f"[{i}/{self.total_tests}] âœ— {test_name} test failed")
            except Exception as e:
                print(f"[{i}/{self.total_tests}] âœ— Test {test_name} exception occurred: {e}")
                self.failed_tests += 1
            
            test_end_time = time.time()
            print(f"Test duration: {test_end_time - test_start_time:.2f} seconds")
            print(f"Current statistics: Passed {self.passed_tests}, Failed {self.failed_tests}")
            
            if i < self.total_tests:
                print("\n" + "-"*40 + " Continue to next test " + "-"*40)
        
        print(f"\nAll tests completed! Final progress: 100.0%")
        print(f"End time: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # æ˜¾ç¤ºæµ‹è¯•ç»“æžœæ‘˜è¦
        self.show_test_summary()
    
    def show_test_summary(self) -> None:
        """Show test result summary"""
        print("\n" + "="*60)
        print("Test Result Summary")
        print("="*60)
        
        print(f"Total tests: {self.total_tests}")
        print(f"Passed tests: {self.passed_tests}")
        print(f"Failed tests: {self.failed_tests}")
        print(f"Success rate: {(self.passed_tests/self.total_tests*100):.1f}%")
        
        print("\nDetailed results:")
        for test_key, result in self.test_results.items():
            status = "âœ“ Passed" if result["success"] else "âœ— Failed"
            exec_time = result["execution_time"]
            description = result["description"]
            print(f"  {description}: {status} (Duration: {exec_time:.2f} seconds)")
        
        # Show failed test details
        failed_tests = {k: v for k, v in self.test_results.items() if not v["success"]}
        if failed_tests:
            print("\nFailed test details:")
            for test_key, result in failed_tests.items():
                print(f"\n{result['description']}:")
                output = result['output'] or "No output information"
                print(f"  Error message: {output[:200]}..." if len(output) > 200 else f"  Error message: {output}")
        
        print("\n" + "="*60)
        
        if self.failed_tests == 0:
            print("ðŸŽ‰ All tests passed! All CHS-SDK simulation modes are working properly.")
        else:
            print(f"âš ï¸  {self.failed_tests} tests failed, please check related configurations and dependencies.")
    
    def save_test_report(self, filename: str = "test_report.txt") -> None:
        """Save test report to file"""
        report_path = self.examples_dir / filename
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("CHS-SDK Simulation Mode Test Report\n")
            f.write("=" * 40 + "\n")
            f.write(f"Test time: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Total tests: {self.total_tests}\n")
            f.write(f"Passed tests: {self.passed_tests}\n")
            f.write(f"Failed tests: {self.failed_tests}\n")
            f.write(f"Success rate: {(self.passed_tests/self.total_tests*100):.1f}%\n\n")
            
            for test_key, result in self.test_results.items():
                f.write(f"{result['description']}:\n")
                f.write(f"  Status: {'Passed' if result['success'] else 'Failed'}\n")
                f.write(f"  Execution time: {result['execution_time']:.2f} seconds\n")
                if not result['success']:
                    f.write(f"  Error message: {result['output']}\n")
                f.write("\n")
        
        print(f"\nTest report saved to: {report_path}")

def main():
    """Main function"""
    tester = SimulationModesTester()
    
    try:
        tester.run_all_tests()
        tester.save_test_report()
        
        # Set exit code based on test results
        sys.exit(0 if tester.failed_tests == 0 else 1)
        
    except KeyboardInterrupt:
        print("\nUser interrupted test")
        sys.exit(1)
    except Exception as e:
        print(f"\nException occurred during testing: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()